# -*- coding: utf-8 -*-
"""GAN_with_PINN2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OPDYZAKfyEgd8unp-g2wsU0mxdn-d3PN

# GAN with PINN
"""

import tensorflow as tf
import os
import time
from IPython import display
import numpy as np
import pandas as pd
from google.colab import drive
import matplotlib.pyplot as plt
from tensorflow import keras
from keras import layers
from sklearn.metrics import f1_score, make_scorer, confusion_matrix, accuracy_score, precision_score, recall_score, precision_recall_curve
from tensorflow.python.ops.numpy_ops import np_config
from google.colab import drive
from keras.layers import Dense, BatchNormalization, LeakyReLU,Reshape
from keras.layers import  Conv2DTranspose, Conv2D, Dropout, Flatten
from tensorflow.python.ops.gen_math_ops import sigmoid
from keras import backend as K
import glob 
import imageio 
import PIL

np_config.enable_numpy_behavior()

drive.mount('/content/gdrive')
DIR ='gdrive/MyDrive/Colab_Notebooks/GAN_with_Pinn/' 
data = np.load("gdrive/MyDrive/Colab_Notebooks/small_data.npy")
labels = np.genfromtxt('gdrive/MyDrive/Colab_Notebooks/all_target.csv', delimiter=',')
labels = labels[1:].astype(int)

dataPINN = [0]*81
for i in range(81):
  dataPINN[i] = [0]*10
for i in range(data.shape[0]):
  for j in range(data.shape[1]):
    dataPINN[i][j] = data[i][j][130:430, 100:400]
plt.imshow(dataPINN[0][0])
plt.show()

labels = np.array([1 if i < 3 else 0 for i in labels])
dataPINN = np.asarray(dataPINN)
dataPINN = dataPINN.reshape(dataPINN.shape[0], 3000, 300).astype('float32')
for i in range(dataPINN.shape[0]):
  dataPINN[i] = dataPINN[i]/dataPINN[i].max()
plt.imshow(dataPINN[0].reshape(10,300,300)[0]) 
plt.show()

test_dataPINN = dataPINN[:21]
test_labels = labels[:21] 
test_labels = test_labels.reshape(21,1)

dataPINN = dataPINN[21:]
labels = labels[21:]
labels = labels.reshape(60,1)

print(test_dataPINN.shape)
print(test_labels.shape)
print(dataPINN.shape)
print(labels.shape)

BUFFER_SIZE = 1000
BATCH_SIZE = 3
RANDOM_SEED = 42

my_labels = tf.data.Dataset.from_tensor_slices([i for i in range(60)])
my_labels = my_labels.shuffle(1000, seed = RANDOM_SEED)
my_labels = my_labels.batch(BATCH_SIZE)

for i in my_labels:
  print(i.tolist(), end = '\t[')
  print(labels[i.tolist()[0]], end = ', ')
  print(labels[i.tolist()[1]], end = ', ')
  print(labels[i.tolist()[2]], end = '] ')
  print()

my_labels = tf.data.Dataset.from_tensor_slices([i for i in range(60)])
my_labels = my_labels.shuffle(1000, seed = RANDOM_SEED)
my_labels = my_labels.batch(BATCH_SIZE)

train_dataset_pinn = tf.data.Dataset.from_tensor_slices(dataPINN);
train_labels = tf.data.Dataset.from_tensor_slices(labels);

train_dataset_pinn = train_dataset_pinn.shuffle(BUFFER_SIZE, seed = RANDOM_SEED)
train_dataset_pinn = train_dataset_pinn.batch(BATCH_SIZE)
train_labels = train_labels.shuffle(BUFFER_SIZE, seed = RANDOM_SEED)
train_labels = train_labels.batch(BATCH_SIZE)

"""## Моделька

"""

def make_generator_model():
    model = tf.keras.Sequential()

    model.add(Dense(27, use_bias=False, input_shape=(100,)))
    model.add(BatchNormalization())
    model.add(LeakyReLU())

    model.add(Dense(16*3, use_bias=True))
    model.add(BatchNormalization())
    model.add(LeakyReLU())

    model.add(Dense(150*15*3, use_bias=True))
    model.add(BatchNormalization())
    model.add(LeakyReLU())
    
    model.add(Reshape((150, 15, 3)))
    assert model.output_shape == (None, 150, 15, 3)
    
    model.add(Conv2DTranspose(3, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 150, 15, 3)
    model.add(BatchNormalization())
    model.add(LeakyReLU())
    
    model.add(Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 300, 30, 3)
    model.add(BatchNormalization())
    model.add(LeakyReLU())
    
    model.add(Conv2DTranspose(2, (5, 5), strides=(5, 5), padding='same', use_bias=False))    
    assert model.output_shape == (None, 1500, 150, 2)
    model.add(BatchNormalization())
    model.add(LeakyReLU())
    
    model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))    
    assert model.output_shape == (None, 3000, 300, 1)

    return model

noise_input = keras.Input(shape=(100,), name="noise")

dense = layers.Dense(27, use_bias=False)(noise_input)
dense = layers.BatchNormalization()(dense)
dense = layers.LeakyReLU()(dense)

param = layers.Dense(50, use_bias=True)(dense)
param = layers.BatchNormalization()(param)
param = layers.Activation(activation='relu', name='Parameters_of_CV')(param)

dense2 = layers.Dense(150*15*3, use_bias=True)(param)
dense2 = layers.BatchNormalization()(dense2)
dense2 = layers.LeakyReLU()(dense2)

dense2 = layers.Reshape((150, 15, 3))(dense2)

conv1 = layers.Conv2DTranspose(3, (5, 5), strides=(1, 1), padding='same', use_bias=False)(dense2)
conv1 = layers.BatchNormalization()(conv1)
conv1 = layers.LeakyReLU()(conv1)

conv2 = layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False)(conv1)
conv2 = layers.BatchNormalization()(conv2)
conv2 = layers.LeakyReLU()(conv2)

conv3 = layers.Conv2DTranspose(2, (5, 5), strides=(5, 5), padding='same', use_bias=False)(conv2)
conv3 = layers.BatchNormalization()(conv3)
conv3 = layers.LeakyReLU()(conv3)

image_pred = layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh', name='Images')(conv3)

generator = keras.Model(
    inputs=[noise_input],
    outputs=[param, image_pred],
)

generator.summary()

keras.utils.plot_model(generator, "generator_model.png",
    show_shapes=False,
    show_dtype=False,
    show_layer_names=True,
    rankdir='TB',
    expand_nested=False,
    dpi=96,
    layer_range=None,
    show_layer_activations=False,
    show_trainable=False
)

noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)[1]
plt.imshow(generated_image[0, :, :, 0], cmap='gray')
plt.show()

image_input = keras.Input(shape=(3000, 300, 1), name="MRI")

conv_disc_1 = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(image_input)
conv_disc_1 = layers.LeakyReLU()(conv_disc_1)
conv_disc_1 = layers.Dropout(0.3)(conv_disc_1)

conv_disc_2 = layers.Conv2D(1, (5, 5), strides=(2, 2), padding='same')(conv_disc_1)
conv_disc_2 = layers.LeakyReLU()(conv_disc_2)
conv_disc_2 = layers.Dropout(0.3)(conv_disc_2)


dense_disc_1 = layers.Flatten()(conv_disc_2)
dense_disc_1 = layers.Dense(27)(dense_disc_1)
dense_disc_1 = layers.LeakyReLU()(dense_disc_1)

param_disc = layers.Flatten()(dense_disc_1)
param_disc = layers.Dense(50)(param_disc)
param_disc = layers.Activation(activation='relu', name='Parameters__Of_CV')(param_disc) #layers.LeakyReLU(name='Parameters__Of_CV')(param_disc) #sigmoid

dense_disc_2 = layers.Flatten()(param_disc)
dense_disc_2 = layers.Dense(10)(dense_disc_2)
dense_disc_2 = layers.LeakyReLU()(dense_disc_2)

diagnosis_output = layers.Flatten()(dense_disc_2)
diagnosis_output = layers.Dense(1)(diagnosis_output)
diagnosis_output = layers.LeakyReLU(name='Diagnosis')(diagnosis_output)


discriminator = keras.Model(
    inputs=[image_input],
    outputs=[param_disc, diagnosis_output],
)

discriminator.summary()

keras.utils.plot_model(discriminator, "discriminator_model.png", show_shapes=False)

S1 = [[1, 2, 0],
 [3, 4, 1],
 [5, 6, 2],
 [4, 6, 7],
 [9, 10, 8],
 [10, 11, 13],
 [12, 9, 14],
 [14, 13, 15]]

S2 = [[0, 15], [12, 3], [11, 5], [7, 8]]

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)
mse = tf.keras.losses.MeanSquaredError()


def loss_CV(real_output, fake_output):
  total_loss = [0]*real_output.shape[0]
  for j in range(real_output.shape[0]):
    total_loss[j] = [0]*(len(S1)*3+len(S2)*2)
    for i in range(len(S2)):
      A1,u1,p1 = real_output[j][S2[i][0]*3], real_output[j][S2[i][0]*3+1], real_output[j][S2[i][0]*3+2]
      A2,u2,p2 = real_output[j][S2[i][1]*3], real_output[j][S2[i][1]*3+1], real_output[j][S2[i][1]*3+2]
      total_loss[j][i*2] = A1*u1 - A2*u2
      if i == 0: # пуазейль на сердце
        total_loss[j][i*2+1] = p1 - p2 - real_output[j][-2] + 0.5*0.105*(u1**2-u2**2)
      else:      # пуазейль на капилляры 
        total_loss[j][i*2+1] = p1 - p2 - real_output[j][-1] + 0.5*0.105*(u1**2-u2**2)
    
    for i in range(len(S1)):
      A1,u1,p1 = real_output[j][S1[i][0]*3], real_output[j][S1[i][0]*3+1], real_output[j][S1[i][0]*3+2]
      A2,u2,p2 = real_output[j][S1[i][1]*3], real_output[j][S1[i][1]*3+1], real_output[j][S1[i][1]*3+2]
      A3,u3,p3 = real_output[j][S1[i][2]*3], real_output[j][S1[i][2]*3+1], real_output[j][S1[i][2]*3+2]
      total_loss[j][8+i*3] = A3*u3 - A2*u2 - A1*u1
      total_loss[j][8+i*3+1] = p3 - p1 + 0.5*0.105*(u3**2-u1**2)
      total_loss[j][8+i*3+2] = p3 - p2 + 0.5*0.105*(u3**2-u2**2)
  answer = mse(total_loss, fake_output)
  return answer


def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

checkpoint_dir = DIR+'training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)

"""## Обучаем"""

EPOCHS = 5
accuracy_cv, f1_cv, precision_cv, recall_cv, g_cv, d_cv, g_cv_cv, d_cv_cv = [], [], [], [], [], [], [], []
num_examples_to_generate = 3 
noise_dim = 100
seed = tf.random.normal([num_examples_to_generate, noise_dim])

@tf.function
def train_step(images, label = None):
    noise = tf.random.normal([BATCH_SIZE, noise_dim])
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
      generated_images = generator(noise, training=True)

      real_output = discriminator(images, training=True)
      fake_output = discriminator(generated_images[1], training=True)

      gen_cv_loss = loss_CV(generated_images[0], tf.zeros((3,32)))
      disc_cv_loss = loss_CV(real_output[0], tf.zeros((3,32))) + loss_CV(fake_output[0], tf.zeros((3,32)))

      gen_loss = generator_loss(fake_output[1])
      disc_loss = discriminator_loss(real_output[1], fake_output[1])

    gradients_of_generator = gen_tape.gradient([gen_loss, gen_cv_loss], generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient([disc_loss, disc_cv_loss], discriminator.trainable_variables)
    
    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

def train(dataset, label, epochs, current):
  for epoch in range(current, epochs):
    start = time.time()
    i = 0
    for image_batch, label_batch in zip(dataset, label):
      train_step(image_batch, label_batch)
      i+=1
      print("Batch",i,"in dataset. Done", time.time()-start)
    generate_and_save_images(generator,epoch + 1,seed)   
    print("Image saved. Done", time.time()-start)
    if (epoch + 1) % 5 == 0:
      checkpoint.save(file_prefix = checkpoint_prefix)
      print("Checkpoint saved. Done", time.time()-start)
  
    noise = tf.random.normal([10, noise_dim])
    generated_images = generator(noise, training=False)
    real_output = discriminator(test_dataPINN, training=False)
    fake_output = discriminator(generated_images[1], training=False)

    gen_loss = generator_loss(fake_output[1])
    disc_loss = discriminator_loss(real_output[1], fake_output[1])

    gen_cv_loss = loss_CV(generated_images[0], tf.zeros((generated_images[0].shape[0],32)))
    disc_cv_loss = loss_CV(real_output[0], tf.zeros((real_output[0].shape[0],32))) + loss_CV(fake_output[0], tf.zeros((fake_output[0].shape[0],32)))

    g_cv.append(gen_loss.numpy())
    d_cv.append(disc_loss.numpy())

    g_cv_cv.append(gen_cv_loss.numpy())
    d_cv_cv.append(disc_cv_loss.numpy())

    print("Metrics evaulated. Gen_Loss: ", gen_loss.numpy(), "Gen_CV: ", gen_cv_loss.numpy(), " Disc_loss: ", disc_loss.numpy(), "Disc_CV: ", disc_cv_loss.numpy(), ". Done", time.time()-start)
    y_tr = np.append(test_labels, np.zeros_like(fake_output[1].numpy())).reshape(31,1)
    y_pr = np.append([0 if i <= 0 else 1 for i in real_output[1].numpy()], [0 if i <= 0 else 1 for i in fake_output[1].numpy()]).reshape(31,1)
    print(accuracy_score(y_true=y_tr, y_pred=y_pr),precision_score(y_tr, y_pr),recall_score(y_tr, y_pr), f1_score(y_tr, y_pr))
    accuracy_cv.append(accuracy_score(y_true=y_tr, y_pred=y_pr))
    precision_cv.append(precision_score(y_tr, y_pr))
    recall_cv.append(recall_score(y_tr, y_pr))
    f1_cv.append(f1_score(y_tr, y_pr))

    with open(DIR+"test.txt", "a") as file_object:
      file_object.writelines("Ep: " + str(epoch+1) + " " + str(accuracy_cv[-1]) + " " + str(f1_cv[-1]) + " " + str(precision_cv[-1]) + " " + str(recall_cv[-1]) + " " + str(g_cv[-1]) + " " + str(d_cv[-1]) + " " + str(g_cv_cv[-1])+ " " + str(d_cv_cv[-1]) + "\n")

    with open(DIR+'g_cv.txt', 'a') as file_object:
      file_object.writelines("Ep: " + str(epoch+1) + " " + str(g_cv[-1]) + "\n")

    with open(DIR+'d_cv.txt', 'a') as file_object:
      file_object.writelines("Ep: " + str(epoch+1) + " " + str(d_cv[-1]) + "\n")

  generate_and_save_images(generator,epochs,seed)

def generate_and_save_images(model, epoch, test_input):
  predictions = model(test_input, training=False)
  with open(DIR+"Params.txt", "a") as file_object:
    file_object.writelines("Ep: " + str(epoch+1) + " " + str(predictions[0].numpy()[0]) + "\n")
  fig = plt.figure(figsize=(18,15))
  l = 0
  cm = 'gray'
  for i in range(predictions[1].shape[0]):
    arr = predictions[1][i, :, :, 0].reshape(10, 300, 300);
    for j in range(10):
      l = l + 1
      plt.subplot(6,5,l)
      plt.imshow(arr[j], cmap=cm)
      plt.axis('off')
  plt.savefig(DIR+'image_at_epoch_{:04d}.png'.format(epoch))

"""## Train(5,0)"""

train(train_dataset_pinn, train_labels, 10, 0)

train(train_dataset_pinn, train_labels, 50, 42)

train(train_dataset_pinn, train_labels, 100, 50)

train(train_dataset_pinn, train_labels, 150, 100)

train(train_dataset_pinn, train_labels, 600, 150)

noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)
print(generated_image[0].numpy()[0])
plt.imshow(generated_image[1][0, :, :, 0], cmap='gray')
plt.show()

decision = discriminator(generated_image[1], training = False)
print(decision[0])
print(decision[1])
decision = discriminator(generated_image[1], training = False)
print(decision[0])
print(decision[1])
decision = discriminator(test_dataPINN[0].reshape(1, 3000, 300, 1), training = False)
print(decision[0])
print(decision[1])

noise = tf.random.normal([21, noise_dim])

generated_images = generator(noise, training=False)
real_output = discriminator(test_dataPINN, training=False)
fake_output = discriminator(generated_images, training=False)
gen_loss = generator_loss(fake_output)
disc_loss = discriminator_loss(real_output, fake_output, test_labels, test_dataPINN)

print(gen_loss, disc_loss)

checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

def display_image(epoch_no):
  return PIL.Image.open(DIR+'image_at_epoch_{:04d}.png'.format(epoch_no))
display_image(EPOCHS)

anim_file = DIR+'dcgan.gif'

with imageio.get_writer(anim_file, mode='I') as writer:
  filenames = glob.glob(DIR+'image*.png')
  filenames = sorted(filenames)
  i = 0
  for filename in filenames:
    i+=1
    print(filename, i)
    image = imageio.imread(filename)
    if i>200:
      writer.append_data(image)
  
display.Image(open(DIR+'dcgan.gif','rb').read())

"""### Графики"""

ac, f1, pr, re, d, g, d_cv, g_cv, d_plus_cv, g_plus_cv = [],[],[],[],[],[],[],[],[],[]

with open('gdrive/MyDrive/Colab_Notebooks/GAN_with_Pinn/test.txt', 'r') as file:
    for line in file:
        lol = line.split()
        ac.append(float(lol[2]))
        f1.append(float(lol[3]))
        pr.append(float(lol[4]))
        re.append(float(lol[5]))
        g.append(float(lol[6]))
        d.append(float(lol[7]))

        g_cv.append(float(lol[8]))
        d_cv.append(float(lol[9]))

        g_plus_cv.append(float(lol[6])+float(lol[8]))
        d_plus_cv.append(float(lol[7])+float(lol[9]))

print(ac, f1, pr, re, d, g, d_cv, g_cv, d_plus_cv, g_plus_cv, sep='\n')
print(len(ac), len(d), len(g))

x = range(1, len(d)+1)
y1 = ac
y2 = f1
y3 = pr
y4 = re
plt.figure(figsize=(12, 7))
plt.title('Accuracy of discriminator. GAN_with_PINN')
plt.plot(x, y1, '-r', label="acc", lw=4, mec='b', mew=2, ms=10)
plt.plot(x, y2, '-g', label="f1", mec='g', lw=2, mew=2, ms=10)
plt.plot(x, y3, '-b', label="prec", lw=2, mec='b', mew=2, ms=10)
plt.plot(x, y4, '-y', label="rec", mec='y', lw=2, mew=2, ms=10)
plt.legend()
plt.grid(True)

x = range(1, len(d)+1)
y1 = d
y2 = g
plt.figure(figsize=(12, 7))
ax = plt.gca()
ax.set_ylim([0, 45])
plt.title('Loss of model GAN_with_PINN (BinaryCrossentropy)')
plt.plot(x, y1, 'r', label="discr_loss", lw=2, mec='b', mew=2, ms=10)
plt.plot(x, y2, 'g', label="gener_loss", mec='r', lw=2, mew=2, ms=12)
plt.legend()
plt.grid(True)

x = range(1, len(d)+1)
y1 = np.log(d)
y2 = np.log(g)
plt.figure(figsize=(12, 7))
plt.title('Loss of model GAN_with_PINN (BinaryCrossentropy)')
plt.plot(x, y1, 'r', label="discr_loss", lw=2, mec='b', mew=2, ms=10)
plt.plot(x, y2, 'g', label="gener_loss", mec='r', lw=2, mew=2, ms=12)
plt.legend()
plt.grid(True)

x = range(1, len(d)+1)
y1 = d_cv
y2 = g_cv
plt.figure(figsize=(12, 7))
plt.title('Residuals of model GAN_with_PINN (PINN)')
plt.plot(x, y1, 'r', label="discr_loss", lw=2, mec='b', mew=2, ms=10)
plt.plot(x, y2, 'g', label="gener_loss", mec='r', lw=2, mew=2, ms=12)
plt.legend()
plt.grid(True)

x = range(1, len(d)+1)
y1 = d_plus_cv
y2 = g_plus_cv
plt.figure(figsize=(12, 7))
plt.title('Loss of model GAN_with_PINN (Combined)')
plt.plot(x, y1, '-r', label="discr_loss", lw=2, mec='b', mew=2, ms=10)
plt.plot(x, y2, '-g', label="gener_loss", mec='r', lw=2, mew=2, ms=12)
plt.legend()
plt.grid(True)

x = range(1, len(d[:300])+1)
y1 = ac[:300]
y2 = f1[:300]
y3 = pr[:300]
y4 = re[:300]
plt.figure(figsize=(12, 7))
plt.title('Accuracy of discriminator. GAN_with_PINN')
plt.plot(x, y1, '-r', label="acc", lw=4, mec='b', mew=2, ms=10)
plt.plot(x, y2, '-g', label="f1", mec='g', lw=2, mew=2, ms=10)
plt.plot(x, y3, '-b', label="prec", lw=2, mec='b', mew=2, ms=10)
plt.plot(x, y4, '-y', label="rec", mec='y', lw=2, mew=2, ms=10)
plt.legend()
plt.grid(True)

x = range(1, len(d[:300])+1)
y1 = d[:300]
y2 = g[:300]
plt.figure(figsize=(12, 7))

ax = plt.gca()
ax.set_ylim([0, 45])
plt.title('Loss of model GAN_with_PINN (BinaryCrossentropy)')
plt.plot(x, y1, 'r', label="discr_loss", lw=2, mec='b', mew=2, ms=10)
plt.plot(x, y2, 'g', label="gener_loss", mec='r', lw=2, mew=2, ms=12)
plt.legend()
plt.grid(True)

x = range(1, len(d[:300])+1)
y1 = d_cv[:300]
y2 = g_cv[:300]
plt.figure(figsize=(12, 7))

ax = plt.gca()
ax.set_ylim([0, 20])
plt.title('Residuals of model GAN_with_PINN (PINN)')
plt.plot(x, y1, 'r', label="discr_loss", lw=2, mec='b', mew=2, ms=10)
plt.plot(x, y2, 'g', label="gener_loss", mec='r', lw=2, mew=2, ms=12)
plt.legend()
plt.grid(True)

x = range(1, len(d[:300])+1)
y1 = d_plus_cv[:300]
y2 = g_plus_cv[:300]
plt.figure(figsize=(12, 7))
ax = plt.gca()
ax.set_ylim([0, 45])
plt.title('Loss of model GAN_with_PINN (Combined)')
plt.plot(x, y1, '-r', label="discr_loss", lw=2, mec='b', mew=2, ms=10)
plt.plot(x, y2, '-g', label="gener_loss", mec='r', lw=2, mew=2, ms=12)
plt.legend()
plt.grid(True)